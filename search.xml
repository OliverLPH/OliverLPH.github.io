<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hexo常用命令]]></title>
    <url>%2F2018%2F12%2F15%2FHexo%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A41%2F</url>
    <content type="text"><![CDATA[Hexo 常用命令（一）命令常用的 hexo简写命令 hexo new draft &quot;new draft&quot; #创建草稿 hexo publish [layout] &lt;filename&gt; #将草稿升级为文章 hexo n &quot;new_post&quot;==hexo new &quot;new_post&quot; #直接创建文章 hexo p==hexo publish #发布文章，将草稿升级为文章 hexo g==hexo generate #生成 hexo s==hexo server #在本地服务器预览，可修改localhost端口hexo d==hexo deploy #部署到远端服务器 hexo clean #清除本地文件]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode解题思路]]></title>
    <url>%2F2018%2F11%2F14%2FLeetCode%E8%A7%A3%E9%A2%98%E6%80%9D%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[Leetcode解题思路 在日常刷LeetCode题时，对自己思路做一些记录语言主要为Python Github上也有我上传好的代码: Github_Peihan’s LeetCode LeetCodeLeetcode #1 Title: Two Sum(两数之和)#1题目描述 代码 1234567891011121314151617181920212223class Solution: def twoSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ for num in nums: remain_num = target - num if remain_num in nums: x = nums.index(remain_num) y = nums.index(num) if x != y: result = [x,y] elif x == y: nums[x] = None if remain_num in nums: y = nums.index(num) result = [x,y] else: pass return result Leetcode #2 Title: Add Two Numbers(两数相加)#2题目描述 代码 1234567891011121314151617181920212223242526# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def addTwoNumbers(self, l1, l2): """ :type l1: ListNode :type l2: ListNode :rtype: ListNode """ dummy = cur = ListNode(0) carry = 0 #进位 while l1 or l2 or carry: if l1: carry += l1.val l1 = l1.next if l2: carry += l2.val l2 = l2.next cur.next = ListNode(carry%10) cur = cur.next carry //= 10 return dummy.next Leetcode #3 Title: Longest Substring Without Repeating Characters(无重复字符的最长子串)#3题目描述 代码 1234567891011121314151617181920class Solution: def lengthOfLongestSubstring(self, s): """ :type s: str :rtype: int """ seen_map = &#123;&#125; max_len = 0 len_last = 0 for i in range(len(s)): l = s[i] if l in seen_map: len_last = min(i - seen_map[l] - 1, len_last) + 1 seen_map[l] = i else: len_last += 1 seen_map[l] = i if len_last &gt;= max_len: max_len = len_last return max_len Leetcode #9 Title: Palindrome Number(回文数)#9题目描述 这道题可以用一句话解决问题。但是仍然把integer转换成了character。 思路如下： 比如输入是 x = 123456 str(x)表示 ‘123456’ str(x)[::-1]表示反向的str(x),就是顺序相反，所以输出为 ‘654321’ 因为回文数的特征，所以直接str(x)==str(x)[::-1]就说明是回文数了 代码 1234567class Solution: def isPalindrome(self, x): """ :type x: int :rtype: bool """ return str(x)==str(x)[::-1] Leetcode #11 Title: Container With Most Water(盛最多水的容器)#11题目描述 这道题目的难点在于时间复杂度，如果直接使用两个循环，那么很容易就超时了。因此我们将思路拓展一下，能够构成最大面积的值是怎么得到的。 比如输入是[1,2,1,4,2,1] 那么此时点的位置则是{(1,1),(2,2),(3,1),(4,4),(5,2),(6,1)} 然后我们就开始从两边的数字可以构成的面积开始推算，然后每次向中间移动一格。此时我们判断构成“容器”的两端的高度，然后将更矮那一端的 垂线 向中间移动，直到两个垂线相邻。 代码 12345678910111213141516171819class Solution: def maxArea(self, height): """ :type height: List[int] :rtype: int """ max_area = 0 #initilize max area i = 0 j = len(height) - 1 while i &lt; j: area = (j-i)*min(height[i], height[j]) max_area = max(max_area, area) if height[i] &lt;= height[j]: i += 1 else: j -= 1 return max_area Leetcode #200 Title: Number of Islands(岛屿的个数)#200题目描述 这题的思路就是使用一个额外的感染函数。遍历数组的时候，当遇到1则将其周围的所有1感染为2，依次感染下去，直到没有1。 代码 12345678910111213141516171819202122232425class Solution: def numIslands(self, grid): if not grid: return 0 count = 0 n = len(grid) m = len(grid[0]) for i in range(n): for j in range(m): if grid[i][j] == '1': grid = self.helper(grid,i,j,n,m) count +=1 else: pass return count def helper(self,grid,i,j,n,m): if i&lt;0 or i &gt;=n or j&lt;0 or j &gt;=m or grid[i][j] != '1': return grid grid[i][j] = '2' self.helper(grid,i,j+1,n,m) self.helper(grid,i+1,j,n,m) self.helper(grid,i-1,j,n,m) self.helper(grid,i,j-1,n,m) return grid]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见机器学习算法]]></title>
    <url>%2F2018%2F07%2F08%2F%E5%B8%B8%E8%A7%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[常见机器学习算法和模型 一般来说我们使用机器学习，就是为了分类。那么对于数据类型的不同，我们就要采用不同的分类方法。 首先说得最多的就是监督学习了。 监督学习(supervised Learning)对于监督学习，我们一般有两个数据集，一个是训练集，一个是数据集。训练集用来训练模型，测试集用来测试模型的准确度。 感知器(Perceptron)和支持向量机(SVM)感知器和支持向量机均是基于线性规划来的。 感知器(Perceptron)感知器的学习是错误驱使的。我们使用感知器的过程中，因为普遍是二分类问题，而且是凸优化，那么我们就通过错分的sample，来不断学习到新的gradient，直到完全正确的分类。当然，我们也可以提前结束学习的过程，因为也有可能遇到outlier，然后严重影响我们的分类 支持向量机(Support vector machine) 它是针对线性可分情况进行分析，对于线性不可分的情况，通过使用非线性映射算法将低维输入空间线性不可分的样本转化为高维特征空间使其线性可分，从而使得高维特征空间采用线性算法对样本的非线性特征进行线性分析成为可能。 它基于结构风险最小化理论之上在特征空间中构建最优超平面，使得学习器得到全局最优化，并且在整个样本空间的期望以某个概率满足一定上界。 HARD MARGIN AND SOFT MARGIN 支持向量机Non-linear SVM支持向量机 Python实现与调参SVM 怎样能得到好的结果 对数据做归一化（simple scaling） 应用 RBF(Radial Basis Function) kernel 用cross-validation和grid-search 得到最优的c和g 用得到的最优c和g训练训练数据 测试 SVM的优缺点优点 解决小样本下机器学习问题。 解决非线性问题。 无局部极小值问题。（相对于神经网络等算法） 可以很好的处理高维数据集。 泛化能力比较强。 缺点 对于核函数的高维映射解释力不强，尤其是径向基函数。 对缺失数据敏感。 决策树决策树的优缺点及改进措施优点 决策树易于理解和解释，可以可视化分析，容易提取出规则。 可以同时处理标称型和数值型数据。 测试数据集时，运行速度比较快。 决策树可以很好的扩展到大型数据库中，同时它的大小独立于数据库大小。 缺点 对缺失数据处理比较困难。 容易出现过拟合问题。 忽略数据集中属性的相互关联。 ID3算法计算信息增益时结果偏向数值比较多的特征。 改进措施 对决策树进行剪枝。可以采用交叉验证法和加入正则化的方法。 使用基于决策树的combination算法，如bagging算法，随即森林(randomforest)算法，可以解决过拟合的问题 逻辑斯特回归(logistic regression)逻辑斯特回归是机器学习中的常见算法，这里我自己总结一下我在学习的过程中总结的知识点。 逻辑回归的定义逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。 逻辑回归的基本假设伯努利分布 $$h_\theta(x;\theta)=p$$ $$p=\frac{1}{1+e^{-\theta^Tx}}$$ $$h_\theta(x;\theta)=\frac{1}{1+e^{-\theta^Tx}}$$ 逻辑回归的损失函数 $$L_\theta(x)=\prod^m_{i=1}h_\theta(x^i;\theta)^{yi} \cdot (1-h_\theta(x^i;\theta))^{1-y^i}$$ 求解逻辑回归 逻辑回归的分类 scikit-learn库中逻辑回归的使用 逻辑斯特回归的优缺点优点 计算代价不高，易于理解和实现 缺点 容易产生欠拟合。 分类精度不高。 朴素贝叶斯(Naive Bayes)朴素贝叶斯的优缺点优点 对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已。 支持增量式运算。即可以实时的对新增的样本进行训练。 朴素贝叶斯对结果解释容易理解。 缺点 由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。 KNNKNN的优缺点优点 KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练 KNN理论简单，容易实现 缺点 对于样本容量大的数据集计算量比较大。 样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多。 KNN每一次分类都会重新进行一次全局运算。 k值大小的选择。 无监督学习(Unsupervised Learning)现实生活中常常会有这样的问题：缺乏足够的先验知识，因此难以人工标注类别或进行人工类别标注的成本太高。很自然地，我们希望计算机能代我们完成这些工作，或至少提供一些帮助。根据类别未知(没有被标记)的训练样本解决模式识别中的各种问题，称之为无监督学习。 聚类(clustering)半监督学习强化学习迁移学习深度学习卷积神经网络(Convolutional nerual network) CNN循环神经网络(Recurrent Neural Network) RNN长短期记忆网络(Long Short-Term Memory) LSTMGated Recurrent Unit(GRU)双向RNN深度神经网络特征工程特征工程在做机器学习的过程中是尤其重要的。因为在现在数据越来越多的情况下，使用正确的特征，是至关重要的。 然后sklearn中也已经包含了许多的方法可以直接调用。 数据的预处理1. 对数据规格的统一，这里有个名词叫（不属于同一量纲）即特征的规格不一样，不能够放在一起比较。&gt; 方法：无量纲化 &gt; 无量纲化使不同规格的数据转换到同一规格。常见的无量纲化方法有标准化和区间缩放法。 &gt;&gt; 标准化的前提是特征值服从正态分布，标准化后，其转换成标准正态分布。 &gt;&gt; 区间缩放法利用了边界值信息，将特征的取值区间缩放到某个特点的范围，例如[0, 1]等。 标准化的方法计算出数据集的均值$ \bar x$和标准差$S $ $$x^{‘}=\frac{x-\bar x}{S}$$ 使用preproccessing库的StandardScaler类对数据进行标准化： 123from sklearn.preprocessing import StandardScaler #标准化，返回值为标准化后的数据 StandardScaler().fit_transform(iris.data) 区间缩放法的方法区间缩放法的思路有多种，常见的一种为利用两个最值进行缩放，公式表达为：$$x^{‘}=\frac{x-Min}{Max-Min}$$ 123from sklearn.preprocessing import MinMaxScaler #区间缩放，返回值为缩放到[0, 1]区间的数据 MinMaxScaler().fit_transform(iris.data) 标准化与归一化的区别简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。规则为l2的归一化公式如下： $$x^{‘}=\frac{x}{\sqrt{\sum_j^m {x[j]}^2}}$$ 123from sklearn.preprocessing import Normalizer #归一化，返回值为归一化后的数据 Normalizer().fit_transform(iris.data) 2. 信息冗余：对于某些定量特征，其包含的有效信息为区间划分，例如学习成绩，假若只关心“及格”或不“及格”，那么需要将定量的考分，转换成“1”和“0”表示及格和未及格。二值化可以解决这一问题。对定量特征二值化定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0，公式表达如下： $$ x^{‘}=\left {\begin{aligned}1,x &gt; threshold \0,x \leq threshold\end{aligned}\right.$$ 使用preproccessing库的Binarizer类对数据进行二值化的代码如下: 123from sklearn.preprocessing import Binarizer #二值化，阈值设置为3，返回值为二值化后的数据 Binarizer(threshold=3).fit_transform(iris.data) 3. 定性特征不能直接使用：某些机器学习算法和模型只能接受定量特征的输入，那么需要将定性特征转换为定量特征。最简单的方式是为每一种定性值指定一个定量值，但是这种方式过于灵活，增加了调参的工作。通常使用哑编码的方式将定性特征转换为定量特征：假设有N种定性值，则将这一个特征扩展为N种特征，当原始特征值为第i种定性值时，第i个扩展特征赋值为1，其他扩展特征赋值为0。哑编码的方式相比直接指定的方式，不用增加调参的工作，对于线性模型来说，使用哑编码后的特征可达到非线性的效果。对定性特征哑编码(one-hot)对平级分类类型的特征采用one-hot编码来进行分类，从而使分类效果不受等级的影响。 1234from sklearn.preprocessing import OneHotEncoder#OneHot编码enc = OneHotEncoder()enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]) 4. 存在缺失值：缺失值需要补充。缺失值计算5. 信息利用率低：不同的机器学习算法和模型对数据中信息的利用是不同的，之前提到在线性模型中，使用对定性特征哑编码可以达到非线性的效果。类似地，对定量变量多项式化，或者进行其他的转换，都能达到非线性的效果。数据变换功能和说明的总结 类 功能 说明 StandardScaler 无量纲化 标准化，基于特征矩阵的列，将特征值转换至服从标准正态分布 MinMaxScaler 无量纲化 区间缩放，基于最大最小值，将特征值转换到[0, 1]区间上 Normalizer 归一化 基于特征矩阵的行，将样本向量转换为“单位向量” Binarizer 二值化 基于给定阈值，将定量特征按阈值划分 OneHotEncoder 哑编码 将定性数据编码为定量数据 Imputer 缺失值计算 计算缺失值，缺失值可填充为均值等 PolynomialFeatures 多项式数据转换 多项式数据转换 FunctionTransformer 自定义单元数据转换 使用单变元的函数来转换数据 特征的选择当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征： 1、特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。 2、特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。 根据特征选择的形式又可以将特征选择方法分为3种： 1、Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。 2、Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。 3、Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。 我们使用sklearn中的feature_selection库来进行特征选择。 Filter 筛选器方差选择法使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。使用feature_selection库的VarianceThreshold类来选择特征的代码如下： 1234from sklearn.feature_selection import VarianceThreshold #方差选择法，返回值为特征选择后的数据 #参数threshold为方差的阈值 VarianceThreshold(threshold=3).fit_transform(iris.data) 相关系数法使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的P值。用feature_selection库的SelectKBest类结合相关系数来选择特征的代码如下： 123456from sklearn.feature_selection import SelectKBest from scipy.stats import pearsonr #选择K个最好的特征，返回选择特征后的数据 #第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数 #参数k为选择的特征个数 SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target) 卡方检验卡方检验就是统计样本的实际观测值与理论推断值之间的偏离程度，实际观测值与理论推断值之间的偏离程度就决定卡方值的大小，卡方值越大，越不符合；卡方值越小，偏差越小，越趋于符合，若两个值完全相等时，卡方值就为0，表明理论值完全符合。 注意：卡方检验针对分类变量。 经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量： $$x^{2}=\sum \frac{({A-E})^2}{E}$$ 这个统计量的含义简而言之就是自变量对因变量的相关性。用feature_selection库的SelectKBest类结合卡方检验来选择特征的代码如下： 1234from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 #选择K个最好的特征，返回选择特征后的数据 SelectKBest(chi2, k=2).fit_transform(iris.data, iris.target) 互信息法经典的互信息也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下： $$ I(X;Y) = \sum_{x \in X} \sum_{y \in Y} p(x,y)log\frac{p(x,y)}{p(x)p(y)}$$ 为了处理定量数据，最大信息系数法被提出，使用feature_selection库的SelectKBest类结合最大信息系数法来选择特征的代码如下： 12345678910from sklearn.feature_selection import SelectKBest from minepy import MINE #由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5 def mic(x, y): m = MINE() m.compute_score(x, y) return (m.mic(), 0.5) #选择K个最好的特征，返回特征选择后的数据 SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target) wrapperEmbeded数据降维主成分分析 Principal Component Analysis(PCA)1234from sklearn.decomposition import PCA #主成分分析法，返回降维后的数据 #参数n_components为主成分数目 PCA(n_components=2).fit_transform(iris.data) 奇异值分解 Singular Value Decomposition(SVD)线性判别分析 Linear Discriminant Analysis(LDA)使用lda库的LDA类选择特征的代码如下： 1234from sklearn.lda import LDA #线性判别分析法，返回降维后的数据 #参数n_components为降维后的维数 LDA(n_components=2).fit_transform(iris.data, iris.target) 特征工程在kaggle的应用自然语言处理]]></content>
      <categories>
        <category>机器学习和数据挖掘</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>机器学习</tag>
        <tag>数据挖掘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见数据挖掘算法]]></title>
    <url>%2F2018%2F07%2F05%2F%E5%B8%B8%E8%A7%81%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[常见数据挖掘算法 这篇文章主要总结了我在学习数据挖掘中的一些知识点 其中大类包含 Association rulesApriori algorithm 先验算法Apriori算法是一种最有影响的挖掘布尔关联规则频繁项集的算法。其核心是基于两阶段频集思想的递推算法。该关联规则在分类上属于单维、单层、布尔关联规则。在这里，所有支持度大于最小支持度的项集称为频繁项集，简称频集。 Park-Chen-Yu algorithmSimple algorithmSimple算法: 从总的Sample里面，随机的取出一部分。 因为对于总的sample，我们取出来的 random sample 始终会有一个 采样频率。我们现在先说 probability p。假如相对于总的篮子 $p=\frac{1}{100}$ 。$\frac{s}{100} $就变成了新的阙值(Threshold). SON algorithmSON算法就是相当于把总的sample分成很多小的chunk，而不是像simple算法一样采样。 SON算法:原理是把本来的bucket分散成很多个chunk，每个chunk包含一定的bucket。再对每一个chunk跑 关联算法（比如A-priori算法）。对于threshold，每一个chunk都有自己新的threshold= s/p 其思想有点类似于 map reduce。正是Map reduce，所以该算法可以用在大数据上。 Clustering聚类是一种无监督学习的算法，最常见的聚类就是使用K-means。 K-meansK-means算法的实质就将不同的sample映射到一个平面上，在这个平面上，不同的sample之间有距离数值上的区别，通过这个区别，就可以清楚的发现这些sample之间的关系。 在选择某种距离作为数据样本间的相似性度量上，有几种常见的距离度量。 欧几里得距离 曼哈顿距离 闵可夫斯基距离 皮尔逊系数(Pearson Correlation Coefficient) K-means的优缺点优点 经典算法，解决聚类问题非常简单和快速。 对处理大数据集，该算法是相对可伸缩和高效率的。因为它的复杂度是0 (n k t ) , 其中, n 是所有对象的数目, k 是簇的数目, t 是迭代的次数。通常k &lt; &lt;n 且t &lt; &lt;n 。 当结果簇是密集的，而簇与簇之间区别明显时, 它的效果较好。 缺点 在簇的平均值被定义的情况下才能使用，这对于处理符号属性的数据不适用。必须事先给出k（要生成的簇的数目）。 K-Means算法对初值敏感，对于不同的初始值，可能会导致不同结果。 它对于“躁声”和孤立点数据是敏感的，少量的该类数据能够对平均值产生极大的影响。 一些改进方法 多设置一些不同的初值，对比最后的运算结果）一直到结果趋于稳定结束，比较耗时和浪费资源 很多时候，事先并不知道给定的数据集应该分成多少个类别才最合适。这也是 K-means 算法的一个不足。有的算法是通过类的自动合并和分裂，得到较为合理的类型数目K. 用更高级的K-means算法，如下 K-prototype算法K-中心点算法K-means++Hierarchical clusteringAgglomerativeDivisivePage RankDecision treeNaive BayesMap Reduce的思想]]></content>
      <categories>
        <category>机器学习和数据挖掘</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>数据挖掘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python语法整理（普通算法，机器学习和数据挖掘的应用）]]></title>
    <url>%2F2018%2F06%2F28%2FPython%E8%AF%AD%E6%B3%95%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[Python语法整理(更新日期 2018/6/28) Python语法整理笔记Python是目前最常用的语言之一，这里我自己总结一下我在学习的过程中遇到的问题。 Python基本语法Python的函数式编程函数式编程的特性 对象，Mutable和Immutable对象 Muatble对象: list, dict, set Immutable对象: strings, tuples, numbers 1.1. 区分Muatble对象和Immutable对象 区别mutable对象和immutable对象的方法很简单，只要看该对象的位置是否发生改变。 查找对象位置id(). 比如 12345678In [1]: a = 1In [2]: id(a)Out[2]: 4345522256In [3]: a = 2In [4]: id(a)Out[4]: 4345522288In [5]: id(2)Out[5]: 4345522288 从这段代码可以看出来，当a=2的时候，id(a)的地址是id(2)的地址，因此a自己是没有创建地址的，只是被指向了2的地址。 &gt;immutable 并不是绝对的不可变。 &gt;&gt; 比如若是tuple里面包含的是一个mutable object，那么mutable object的值仍然是可变的。但是因为tuple 中的 mutable object 改变的只是 value而不是identity。因此tuple仍然是immutable的object。 1.2 immutable data的作用 像Clojure一样，默认上变量是不可变的，如果你要改变变量，你需要把变量copy出去修改。这样一来，可以让你的程序少很多Bug。因为，程序中的状态不好维护，在并发的时候更不好维护。（你可以试想一下如果你的程序有个复杂的状态，当以后别人改你代码的时候，是很容易出bug的，在并行中这样的问题就更多了） first class functions可以理解为面向对象编程 尾递归优化Python不支持尾递归优化 函数式编程的方法 经典函数: Map &amp; Reduce 在Python中，Map reduce用的非常之多 举几个简单的例子 123&gt;&gt; length = map(len,[&quot;123&quot;,&quot;425&quot;,&quot;abc&quot;])&gt;&gt; print(list(length))&gt;&gt; [3, 3, 3] #输出 Map函数还可以和lambda函数一起使用 123&gt;&gt; square_list = map(lambda x:x*x, [1,2,3,4,5,10])&gt;&gt; print(list(square_list))&gt;&gt; [1, 4, 9, 16, 25, 100] #输出 Map函数用来批量开根号 123&gt;&gt; square_root_list = map(lambda x:pow(x,0.5), [4,9,16,25,36,2,3])&gt;&gt; print(list(square_root_list))&gt;&gt; [2.0, 3.0, 4.0, 5.0, 6.0, 1.4142135623730951, 1.7320508075688772] #输出 然后就是reduce函数怎么用 在Python3中，reduce() 函数已经被从全局名字空间里移除了，它现在被放置在 fucntools 模块里，如果想要使用它，则需要通过引入functools模块来调用 reduce()函数： 举个例子 12345&gt;&gt; #必须要先从functool里面调用才可以用&gt;&gt; from functools import reduce&gt;&gt; factorial = reduce(lambda x, y: x+y, [1, 2, 3, 4, 5])&gt;&gt; print(factorial)&gt;&gt; 15 #输出 这个例子相当于是计算了((1+2)+3)+4)+5),就是叠加了起来。其中 reduce(function, iterable[, initializer]), fucntion函数必须要有两个入参，每次先从第一二个参数开始运算function，然后得出了结论，就可以和第三个参数继续运算。因此符合了Reduce的思想，将运算分割开来了。 因为reduce的特性，阶乘，叠加，求和等思路的的可以用用reduce。 1234&gt;&gt; from functools import reduce&gt;&gt; itera = reduce(lambda x,y: x*y, [1,2,3,4,5,6])&gt;&gt; print(itera)&gt;&gt; 720 #输出 上面的代码就是输出了6的阶乘 参考网页: 函数式编程这个例子是从该网页看到的 还有一些别函数filter, find, all, any(其它函数式的语言也有),可以让你的代码更简洁,更清楚。 计算数组中正数的平均值 123456789101112num =[2, -5, 9, 7, -2, 5, 3, 1, 0, -3, 8]positive_num_cnt = 0positive_num_sum = 0for i in range(len(num)): if num[i] &gt; 0: positive_num_cnt += 1 positive_num_sum += num[i] if positive_num_cnt &gt; 0: average = positive_num_sum / positive_num_cnt print(average) #输出5 如果采用函数式编程 12positive_num = filter(lambda x: x&gt;0, num)average = reduce(lambda x,y: x+y, positive_num) / len( positive_num ) 如上面的代码所示，两句话就把这个问题解决了 Pipeline Recursing currying higher order function 函数式编程的好处 parallelization 并行 lazy evaluation 惰性求值 determinism 确定性 Python数据结构堆 ADTpython的堆，可以用内置库实现priority queue algorithm优先级队列 1234567891011121314151617181920212223242526import heapq#内置堆默认为小根堆#初始化堆PriorityQueue = [] #用列表来做储存的数据结构#增，往堆里添加item#heapq.heappush(heap, item)heapq.heappush(PriorityQueue,6)heapq.heappush(PriorityQueue,1)heapq.heappush(PriorityQueue,3)heapq.heappush(PriorityQueue,4)heapq.heappush(PriorityQueue,5)print(PriorityQueue)#以上的代码往堆里压入了6，1，3，4，5#弹出堆里的最小值#heapq.heappop(heap)min_heap = heapq.heappop(PriorityQueue)print(min_heap)print(PriorityQueue)#push和pop两个操作一起进行,push item进去，pop最小值出来#heapq.heappushpop(heap, item)#该操作效率相比于 分别进行 push &amp; pop更高min_heap2 = heapq.heappushpop(PriorityQueue, 100)print(min_heap2)print(PriorityQueue) 堆排序也可以基于内置的heapq库来进行 123456789#堆排序def heapsort(iterable): h = [] for value in iterable: heapq.heappush(h, value) return [heapq.heappop(h) for i in range(len(h))]list_ = [100,200,50,4,15,666,333]print(heapsort(list_)) Python模块常用语法正则表达式正则表达式是一个非常常用的模块，然后，可以模糊的匹配出所需要的模组 1234import reregex = &lt;正则表达式&gt;info = '&lt;&gt;'(str)re.match(regex,info) 语法比较简单，但是里面匹配的时候，有一些特殊符号的作用 ()只负责提取括号内的内容 group(0): 代表提取出最外层括号的内容 group(1): 代表提取出第一个括号内的内容，然后数字叠加以此内推 ^ 限定开头 $ 限定结尾 * 限定字符出现任意多次 . 表示任意字符 ? 取消贪婪模式，从左往右寻找数字 + 限定字符多出现1次 {2} {2,5} []: 中括号的内容中括号有三个用途 区间， 任意字符出现 [.*]不包含其他含义的字符，中括号里面的.就是普通的点 中括号里只要出现任意一个，都是可以匹配到的。常用在匹配电话号码中。比如任意 电话 150 565 56735 130 565 56735 137 565 56735模糊匹配 1[357]0 565 56735可以把他们都匹配得到 \s 表示空格 \S 表示不为空格都可以,但是只能表示一个字符 \w 表示[0-9,a-z,A-Z,_这些字符] （数字，字符，下划线） \W 表示非以上字符 [\u4E00-\u9FA5]汉字区间，表示所有的汉字，均可以匹配出来 \d 表示数字 Python算法]]></content>
      <categories>
        <category>机器学习和数据挖掘</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>机器学习</tag>
        <tag>数据挖掘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL语法整理]]></title>
    <url>%2F2018%2F06%2F23%2FSQL%E8%AF%AD%E6%B3%95%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[SQLSQL的用途十分广泛，其实很多时候也挺复杂的，我的SQL水平并不太好，然后就在这里总结一下除开增，删，改，查等基本操作的常用的一些SQL写法。 其中也会穿插一些在Leetcode上面做的SQL的题目，和自己的思路。 内置函数聚合函数 GROUP BYGROUP BY 语句用于结合聚合函数，根据一个或多个列对结果集进行分组。 SQL GROUP BY 语法 代码 1234SELECT column_name, aggregate_function(column_name)FROM table_nameWHERE column_name operator valueGROUP BY column_name; 实例演示 Student Class Age Gender Oliver A 19 M Allie A 18 F Lee B 20 M Jerry A 19 M Frank B 18 F 12SELECT Class,AVG(Age) as Avg_age FROM Classes GROUP BY Class; 以班级为聚合目标来进行聚合，求出班级的平均年龄得到下表。 Class Avg_age A 18.6667 B 19.0000 HAVING 子句在 SQL 中增加 HAVING 子句原因是，WHERE 关键字无法与聚合函数一起使用。HAVING 子句可以让我们筛选分组后的各组数据。 SQL HAVING 语法 代码 12345SELECT column_name, aggregate_function(column_name)FROM table_nameWHERE column_name operator valueGROUP BY column_nameHAVING aggregate_function(column_name) operator value; 参考网址：菜鸟教程SQL Having语法 实例演示 “Websites”表的数据 id name url alexa country 1 Google https://www.google.cm/ 1 USA 2 淘宝 https://www.taobao.com/ 13 CN 3 菜鸟教程 http://www.runoob.com/ 4689 CN 4 微博 http://weibo.com/ 20 CN 5 Facebook https://www.facebook.com/ 3 USA 7 stackoverflow http://stackoverflow.com/ 0 IND “access_log”网站访问记录表的数据 aid site_id count date 1 1 45 2016-05-10 2 3 100 2016-05-13 3 1 230 2016-05-14 4 2 10 2016-05-14 5 5 205 2016-05-14 6 4 13 2016-05-15 7 3 220 2016-05-15 8 5 545 2016-05-16 9 3 201 2016-05-17 现在我们想要查找总访问量大于 200 的网站。我们使用下面的 SQL 语句： 12345SELECT Websites.name, Websites.url, SUM(access_log.count) AS nums FROM (access_logINNER JOIN WebsitesON access_log.site_id=Websites.id)GROUP BY Websites.nameHAVING SUM(access_log.count) &gt; 200; 得出了访问量大于200的网站 name url nums Facebook https://www.facebook.com/ 750 Google https://www.google.cm/ 275 菜鸟教程 http://www.runoob.com/ 521]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop(3.0.3)安装和配置过程（Mac）]]></title>
    <url>%2F2018%2F06%2F23%2FHadoop%E9%85%8D%E7%BD%AE%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[最近想研究一下分布式计算工具，于是瞄上了Hadoop，在网上找了很多教程，但还是遇到了一些问题，在这里就我记录一下遇到的问题。版本是 Hadoop(3.0.3) Hadoop(3.0.3) Java环境检查Java版本号检查在terminal里输入如下指令可以查看Java版本1$ java -version 如果Java已经是安装好了的话，就会出现如下的信息，里面的版本号可能会不一样 1234$ java -versionjava version &quot;1.8.0_161&quot;Java(TM) SE Runtime Environment (build 1.8.0_161-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode) Java安装如果你没有Java，在这网址里可以安装Java安装网址JDK的安装就搜索一下就有 Java的的系统环境变量Java的系统环境变量是如下指令查询的1$ /usr/libexec/java_home 查询成功会反馈类似的路径1/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home 该路径一会配置环境变量会用得上 Hadoop下载Hadoop下载链接我是在这里下载的3.0.3版本号。和之前2.x的版本有些许不一样。然后我是下载的编译好的binary版本。Hadoop3.0.3下载链接进来之后任意选择镜像下载就可以了。 然后把解压后的文件夹hadoop-3.0.3存放到一个路径，比如我就把该文件夹放在documents下面。路径如下1/Users/Oliver/Documents/hadoop-3.0.3 配置Mac os环境SSH这里就是我遇到问题最奇怪的地方 按照别人讲的教程大概有如下三种方法1$ ssh localhost 然后查看是否登陆成功,登陆成功讯息如下123$ ssh localhostMBP:hadoop-3.0.3 Oliver$ ssh localhostLast login: Sat Jun 23 10:42:58 2018 如果这里不是直接输出如上讯息，那么看是否需要输入密码，就根据提示一步一步来。如果还是不成功，那么看如下方法 方法一 打开系统偏好设置 –&gt; 共享–&gt; 勾选远程登录–&gt; 设置允许访问：所有用户 然后再次在terminal里输入1$ ssh localhost 参考网址：Mac 系统安装Hadoop 2.7.3参考网址：Setting up Hadoop 2.6 on Mac OS X Yosemite 如果以上方法不成功 方法二生成一个ssh key并储存和拷贝，方法比较笨 先输入如下指令，生成ssh key 1$ ssh-keygen 然后会提示将该key储存在哪里，这个路径就随意设置,比如我就储存在hadoop文件夹内 1/Users/Oliver/Documents/hadoop-3.0.3/sshkey/id_rsa 其中id_rsa就是我们要的文件。 键入如下指令 1cp /Users/Oliver/Documents/hadoop-3.0.3/sshkey/id_rsa.pub .ssh/authorized_keys 其中路径要和我们储存key的路径一致。这是为了拷贝新生成的public key 最后我们再试着输入 1$ ssh localhost 此时应该就可以成功了 参考网址：Hadoop: start-dfs.sh permission denied 方法三此方法还未经过验证（未完成） 配置环境变量总环境变量配置打开terminal 12$ touch ~/.bash_profile$ vim ~/.bash_profile 执行以上代码可以进入Vim编辑器，编辑环境变量。添加如下代码键入 i 可以开始编辑（详情可以搜索Vim的命令）123# Hadoopexport HADOOP_HOME=/Users/Oliver/Documents/hadoop-3.0.3(这个是Hadoop distribution的路径，根据自己设定的来调)export PATH=$PATH:$HADOOP_HOME/bin 然后保存再退出，以下是部分Vim的指令，其余指令可以自行搜索一下 命令 简单说明 :w 保存编辑后的文件内容，但不退出vim编辑器 :wq 保存文件内容后退出vim编辑器 :q 在未做任何编辑处理而准备退出vim时，可以使用此命令 退出Vim界面之后，在terminal里输入如下命令使变量立即生效1$ source ~/.bash_profile 我们就成功的配置好了Hadoop1$ hadoop version 用该语句来确认Hadoop配置,此时可以查看 Hadoop文件配置打开如下路径里的文件进行更改, 将代码复制进去 etc/hadoop/core-site.xml: 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; etc/hadoop/hdfs-site.xml: 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; etc/hadoop/mapred-site.xml: 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; etc/hadoop/yarn-site.xml: 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 参考网址：Setting up Hadoop 2.6 on Mac OS X Yosemite /etc/hadoop/hadoop-env.sh handoop环境添加这段代码就可以了，注意就是Java的系统环境变量的路径 1export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home 可以看到前面一开始提到的用如下指令查询 1$ /usr/libexec/java_home 执行 start hdfs and yarn 在terminal里进入hadoop distribution的文件夹 12$ cd 到hadoop的文件夹，比如下面路径就是我的Hadoop$ cd /users/Oliver/documents/hadoop-3.0.3/ 对Namenode执行格式化操作 1$ hadoop namenode -format 启动hadoop 启动NameNode和DataNode 1$ sbin/start-dfs.sh 成功后出现 123Starting namenodes on [localhost]Starting datanodesStarting secondary namenodes [xiaohuobandeMBP.local] 启动ResourceManager和NodeManager 1$ sbin/start-yarn.sh 成功后出现 12Starting resourcemanagerStarting nodemanagers 检查hadoop是否启动成功 在terminal里输入 1$ jps 会显示 1234518739 Jps18133 NameNode18375 SecondaryNameNode18568 ResourceManager18665 NodeManager 则表示启动成功。 此时在浏览器中访问localhost:8088和localhost:9870(hadoop3.x版本)，可以看到Hadoop的界面 localhost:50070(Hadoop2.x版本) 到此为止，Hadoop就已经完成配置了，接下来就是使用Hadoop了]]></content>
      <categories>
        <category>机器学习和数据挖掘</category>
      </categories>
      <tags>
        <tag>数据挖掘</tag>
        <tag>Hadoop</tag>
        <tag>分布式计算</tag>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo deploy问题]]></title>
    <url>%2F2018%2F06%2F22%2Fhexo-deploy%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Hexo deploy的问题一开始deploy成功之后，再重新deploy的时候，会发现并没有推送本地的文件到GitHub 12345678910111213INFO Deploying: gitINFO Clearing .deploy_git folder...INFO Copying files from public folder...INFO Copying files from extend dirs...On branch masternothing to commit, working tree cleanTo github.com:OliverLPH/OliverLPH.github.io + a8ea0df...bc3e952 HEAD -&gt; master (forced update)Branch &apos;master&apos; set up to track remote branch &apos;master&apos; from &apos;git@github.com:OliverLPH/OliverLPH.github.io&apos;.INFO Deploy done: gitxiaohuobandeMBP:source Oliver$ hexo cleanINFO Deleted database.INFO Deleted public folder. 然后如上面的代码所示，hexo显示nothing to commit, working tree clean。 在网上搜索后发现有的几种方案 方案一12hexo cleanhexo d 这样就可以清理掉public文件夹，然后重新deploy 方案二移除.deploy文件夹123rm -rf .deployhexo generaterhexo deploy]]></content>
      <categories>
        <category>Hello World!</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>初次使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World！]]></title>
    <url>%2F2018%2F06%2F22%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Hello World!终于把个人博客弄好的，菜鸟真是不容易。还是要感谢一下网上的各路大神，都是借鉴了大家的攻略才终于弄好的。 参考网站Hexo主题来源: Next主题Hexo配置参考1: Hexo搭建独立博客全纪录Hexo配置参考2: Hexo官方文档Hexo配置参考3: hexo博客更换主题]]></content>
      <categories>
        <category>Hello World!</category>
      </categories>
      <tags>
        <tag>初次使用</tag>
      </tags>
  </entry>
</search>
